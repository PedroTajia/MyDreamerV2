MyDreamerV2
A PyTorch re-implementation of DreamerV2 for pixel-based control.
Built around an RSSM world model, actor‚Äìcritic learning in imagination, and practical tooling for training and logging. The repo currently includes minimal components (RSSM, encoder/decoder, reward/value/discount models, actor, replay buffer, and a RoboSuite gym wrapper).

## ‚ú® Highlights
PyTorch-first code with small, readable modules (RSSM, encoders/decoders, actor, buffers). 
GitHub
Pixel-based continuous control setup (e.g., RoboSuite Lift) via a custom Gym wrapper. 
GitHub
End-to-end world model training (reconstruction, reward, continuation) + imagined rollouts for policy/value learning, following DreamerV2. 

## üì¶ Installation
Tested with Python 3.10+ and PyTorch. If you‚Äôre on macOS (MPS) or CUDA, install the appropriate PyTorch build from pytorch.org first.


# 1) Install core deps
`pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu`
`pip install gymnasium pygame imageio tqdm numpy matplotlib pillow`

# 2) RoboSuite (for pixel control tasks)
`pip install robosuite==1.4.1`

# 3) (Optional) experiment tracking
`pip install wandb`
DreamerV2 background: paper and project page for conceptual details. 

üöÄ Quick Start
The repository ships with a simple training script and minimal components. A default run (with the included environment wrapper) looks like:
python run.py

Environment: task name, robot arm, reward shaping, frame size. (See env.py.) 

Model: RSSM sizes, stochastic/deterministic dims, encoder/decoder depth. (See rssm.py, image_codec.py.) 

Learning: batch size, sequence length, KL scale, actor/value learning rates. (See train.py, networks.py, actor.py.) 

Replay: capacity, sampling, episodic slicing. (See buffer.py.) 

If you use Weights & Biases:
`export WANDB_PROJECT=mydreamerv2`
python run.py
üß± Repository Structure
MyDreamerV2/
‚îú‚îÄ run.py            # Entry point for training/eval loops
‚îú‚îÄ train.py          # World model + actor/value training steps
‚îú‚îÄ env.py            # Gym wrapper for RoboSuite pixel observations
‚îú‚îÄ image_codec.py    # CNN Encoder / Decoder for pixels
‚îú‚îÄ rssm.py           # Recurrent State-Space Model (RSSM)
‚îú‚îÄ networks.py       # Reward, Value, Discount heads
‚îú‚îÄ actor.py          # Policy (actor) + action distribution
‚îú‚îÄ buffer.py         # Replay buffer / episodic sampling

GitHub
üß† Method (DreamerV2, in short)
DreamerV2 learns a discrete/structured latent world model (stochastic + deterministic states), optimizes reconstruction/reward/continuation losses with a KL regularizer, and then trains an actor‚Äìcritic purely from imagined trajectories rolled out in latent space. This decouples representation learning from control while remaining sample-efficient. 

‚öôÔ∏è Configuration Tips
Image size & channels: Keep encoder/decoder resolutions consistent across env.py and image_codec.py.
Sequence length: Long enough for credit assignment through imagination (e.g., 50‚Äì80), but balanced against memory.
KL balancing: If reconstructions look good but imagination is unstable, tune KL scale or free nats.
Action distribution: For continuous control, a squashed Gaussian (TanhNormal) is typical; ensure bounds match the env.

üìä Logging & Visualization
Metrics (losses, returns, KL, reconstruction MSE) and media (episode videos) can be logged to W&B.
For videos, stack frames as np.uint8 [T, H, W, 3] and log every N steps to avoid I/O slowdown.

